<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping">
  <meta name="keywords" content="Skip-Vision, Efficiency, Scalability, Training time, FLOPs, Latency">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<style>
    .pulse {
        animation: pulse 2s infinite;
    }
    .highlighted {
        font-size: 1.5em;
        color: #e74c3c;
        font-weight: bold;
        margin-bottom: 0.5em;
      }
    @keyframes pulse {
        0% { transform: scale(1); box-shadow: 0 0 0 0 rgba(255, 87, 34, 0.7); }
        70% { transform: scale(1.05); box-shadow: 0 0 0 10px rgba(255, 87, 34, 0); }
        100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(255, 87, 34, 0); }
    }
</style>
<style>
    .small-image {
        width: 200px; 
        height: auto; 
    }
    .responsive-image {
        max-width: 80%; 
        height: auto;  
    }
</style>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping</h1>
          <div class="is-size-5 publication-authors">
            <div class="highlighted">ICCV 2025</div>
            <span class="author-block">
              Weili Zeng</a><sup>1</sup>,</span>
            <span class="author-block">
              Ziyuan Huang</a><sup>2</sup>,</span>
            <span class="author-block">
              Kaixiang Ji</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://daodaofr.github.io/">Yichao Yan</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University</span>
            <span class="author-block"><sup>2</sup>Ant Group</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.21817"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zwl666666/Skip-Vision"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/framework-new.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <b>The framework of Skip-Vision.</b> a) While visual scaling enriches visual information, it also increases computational overhead. Skip-Vision uses a skip-FFN strategy during training to reduce redundant computation for visual tokens. The numerous skipped tokens will be limited to the attention layer and bypass FFN. b) At the beginning of inference, Skip-Vision will remove skip-FFN visual tokens from the initial KV-cache, enhancing efficiency. c) During inference, skip attention leverages the skip KV-cache to accelerate generation.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. 
            For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. 
          </p>
            Experimental results demonstrate that Skip-Vision reduces training time by up to 35%, inference FLOPs by 75%, and latency by 45%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <head>
    <!-- å¼•å…¥ Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>

    <!-- Highlights. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Highlights</h2>
        <div class="content has-text-justified">
          Skip-Vision is designed to be seamlessly integrated into the standard SFT pipeline of MLLMs without introducing additional re-training or decoupled modules. It directly modifies the 
          transformerâ€™s computation flow (skip-FFN + token merge + adaptive summary token) and inference (skip-KV cache), offering a practical and theoretically grounded acceleration solution for MLLM training and inference jointly.
          <p>
            <b><span style="color: #FF9800; font-size: 30px;">ðŸš€</span>Training speedup.</b> We introduce a novel and efficient Skip-Vision framework, using token merge and skip FFN strategy during training to reduce redundant computation for visual tokens. 
          </p>
          <p>
            <b><span style="color: #FF9800; font-size: 30px;">ðŸš€</span>Inference speedup.</b> In inference, Skip-Vision employs a skip KV-cache mechanism that removes skip-FFN visual tokens from the KV-cache, enhancing efficiency. 
          </p>
          <p>
            <b><span style="font-size: 24px;">ðŸ˜Š</span>Performance.</b> Experiments show Skip-Visionâ€™s superior efficiency, effective data scaling, and performance on par with state-of-the-art models of similar scale.
          </p>
        </div>
        <img src="./static/images/tradeoff-new.png"
                 class="responsive-image"
                 alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            <b>Performance-efficiency trade-off curve.</b> Each circle denotes a model configuration, where our models utilize
              the Skip-Vision framework, with CoS, LLava-HD and LLaVA serving as baselines. Circle sizes reflect the inference FLOPs ratio. Skip-Vision demonstrate superior performance,
              scaling effectively with increased FLOPs and data and achieving higher inference efficiency when compared to baselines and other effcient MLLM methods. All methods utilize LLaMA3 8B as the
              foundational large language model.
          </p>
        </div>
      </div>
    </div>
    <!--/ Highlights. -->

    <!-- Performance. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance</h2>
        <div class="content has-text-justified">
          MMVet, MMStar and MMBench highlight Skip-Visionâ€™s strength in capturing causal
          and global information. These benchmarks emphasize
          high-level reasoning and abstraction, which benefit from
          Skip-Visionâ€™s ability to preserve essential information
          flow while reducing redundant computations. By skipping
          FFN and KV-cache for less informative tokens, the model
          amplifies signal from key visual cues and enhances causal
          token interactions. While this comes with a slight trade off in fine-grained tasks (OCR, Textvqa), it reflects a deliberate balance between perception and reasoning, favoring
          tasks that rely on semantic integration over detail fidelity.
        </div>
          <img src="./static/images/performance1.png"
                 class="responsive-image"
                 alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            We evaluate Skip-Vision on LLaVA, LLaVA-HD, and CoS and compare it with state-ofthe-art efficiency optimization models under the LLaVA setting (LLaMA3 8B). Nr and Ns denote the number of retained and skipped tokens, respectively
          </p>
        </div>
          <img src="./static/images/performance4.png"
                 class="responsive-image"
                 alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            Following the LLaVA-1.5-7B training setup, we conducted additional comparisons between Skip-Vision and several recent works.
          </p>
        </div>
          <img src="./static/images/performance3.png"
                 class="responsive-image"
                 alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            We present the performance of SV-CoS on SV-9M, comparing it against the current SOTA models of a similar scale.
          </p>
        </div>
      </div>
    </div>
    <!--/ Performance. -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zeng2025skipvisionefficientscalableacceleration,
      title={Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping}, 
      author={Weili Zeng and Ziyuan Huang and Kaixiang Ji and Yichao Yan},
      year={2025},
      eprint={2503.21817},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.21817}, 
}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/Skip-Vision.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/zwl666666/Skip-Vision" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
